# Lab 5 Part 4: NER vá»›i Bi-LSTM


## 1. ğŸ—‚ï¸ Cáº¥u trÃºc file code
```
â”œâ”€â”€ Import thÆ° viá»‡n (torch, datasets, seqeval) & Ä‘á»‹nh nghÄ©a siÃªu tham sá»‘ 
â”œâ”€â”€ Task 1: Load dataset CoNLL-2003 + xÃ¢y vocab thá»§ cÃ´ng 
â”œâ”€â”€ Task 2: Dataset class + collate_fn (padding) 
â”œâ”€â”€ Task 3: Äá»‹nh nghÄ©a model Bi-LSTM (khÃ´ng tÃ­ch há»£p CRF) 
â”œâ”€â”€ Task 4: VÃ²ng láº·p train (CrossEntropyLoss) + Early Stopping 
â”œâ”€â”€ Task 5: Dá»± Ä‘oÃ¡n thá»­ nghiá»‡m (Inference) 
â””â”€â”€ HÃ m evaluate chi tiáº¿t (SeqEval metrics)
```
## 2. ğŸ”§ Chi tiáº¿t tá»«ng pháº§n triá»ƒn khai

### **1. Imports & Hyperparameters**
- PyTorch core, HuggingFace datasets, seqeval.
- Tham sá»‘ chÃ­nh:
- `BATCH_SIZE = 16`
- `EMBEDDING_DIM = 100`
- `HIDDEN_DIM = 256`
- `PATIENCE = 3`

### **2. Load dá»¯ liá»‡u**
- Dataset: **CoNLL-2003**.
- XÃ¢y **vocab thá»§ cÃ´ng** tá»« táº­p train.
- Token OOV â†’ `<UNK>`.


### **3. Dataset + DataLoader**
- Class `NERDataset` káº¿ thá»«a `torch.utils.data.Dataset`.
- HÃ m `collate_fn` dÃ¹ng `pad_sequence(batch_first=True)`.
- Padding:
- Token = `0`
- Label = `PAD_TAG = -1`


### **4. Bi-LSTM Model**
- Kiáº¿n trÃºc model:
- `nn.Embedding(vocab_size, 100)`
- `nn.LSTM(..., bidirectional=True, batch_first=True)`
- `nn.Linear(hidden_dim * 2, num_labels)`
- KhÃ´ng cÃ³ lá»›p CRF.


### **5. Huáº¥n luyá»‡n**
- Loss: `nn.CrossEntropyLoss(ignore_index=PAD_TAG)`.
- Optimizer: Adam.
- Early Stopping theo `val_loss` (patience = 3).


### **6. ÄÃ¡nh giÃ¡**
- Metrics:
    - Loss
    - Token-level Accuracy
    - Entity-level Precision/Recall/F1 (seqeval)


### **7. Dá»± Ä‘oÃ¡n**
- `predict_sentence()`:
- Preprocess â†’ Model â†’ Argmax.


## 3. ğŸ“Š Káº¿t quáº£ huáº¥n luyá»‡n & Ä‘Ã¡nh giÃ¡


### **Káº¿t quáº£ huáº¥n luyá»‡n**


| Epoch | Train Loss | Val Loss | Val Acc | Ghi chÃº |
|-------|-----------|----------|----------|---------|
| 1 | 0.3705 | 0.2642 | 0.9262 | Loss giáº£m nhanh |
| 2 | 0.1588 | 0.2051 | 0.9431 | Há»c tá»‘t |
| 3 | 0.0716 | 0.1841 | 0.9483 | **Best model** |
| 4 | 0.0270 | 0.1942 | 0.9519 | Overfitting nháº¹ |
| 5 | 0.0089 | 0.2128 | 0.9525 | Overfitting máº¡nh |
| 6 | 0.0035 | 0.2415 | 0.9526 | Early Stopping |


---


### **Káº¿t quáº£ dá»± Ä‘oÃ¡n thá»±c táº¿**
- **Æ¯u Ä‘iá»ƒm:**
- Nháº­n diá»‡n tá»‘t cÃ¡c thá»±c thá»ƒ rÃµ rÃ ng: *New York City (LOC), Microsoft (ORG)*.
- **NhÆ°á»£c Ä‘iá»ƒm (thiáº¿u CRF):**
- Sai logic nhÃ£n: *I-PER Ä‘á»©ng Ä‘áº§u chuá»—i*.
- Miss entity: *Malala â†’ O*.


---


### **SeqEval (Test Set)**


| Entity | Precision | Recall | F1 | Support |
|--------|-----------|--------|-----|----------|
| LOC | 0.86 | 0.68 | 0.76 | 1668 |
| MISC | 0.45 | 0.64 | 0.53 | 702 |
| ORG | 0.74 | 0.57 | 0.64 | 1661 |
| PER | 0.65 | 0.67 | 0.66 | 1617 |
| **Macro Avg** | **0.68** | **0.64** | **0.65** | **5648** |


ğŸ” **Nháº­n xÃ©t chuyÃªn mÃ´n:**
- ORG recall tháº¥p â†’ model bá» sÃ³t nhiá»u tá»• chá»©c.
- MISC precision tháº¥p â†’ model dá»± Ä‘oÃ¡n nháº§m nhiá»u.
- F1 tá»•ng chá»‰ ~0.66 â†’ má»©c trung bÃ¬nh cho mÃ´ hÃ¬nh khÃ´ng cÃ³ pretrained embedding.


---


## 4. âš ï¸ Háº¡n cháº¿ & Váº¥n Ä‘á» ká»¹ thuáº­t


### **1. Thiáº¿u CRF Layer**
- CrossEntropyLoss khÃ´ng há»c Ä‘Æ°á»£c **transition rules**.
- Dáº«n Ä‘áº¿n chuá»—i nhÃ£n khÃ´ng há»£p lá»‡.


### **2. Embedding ngáº«u nhiÃªn**
- KhÃ´ng dÃ¹ng GloVe/BERT â†’ mÃ´ hÃ¬nh há»c cháº­m vÃ  yáº¿u.


### **3. Tokenization & OOV**
- Word-level thá»§ cÃ´ng â†’ máº¥t thÃ´ng tin á»Ÿ entity hiáº¿m.

## 5. ğŸ“š TÃ i liá»‡u 
- Notebook: *lab5-part4.ipynb*
- Dataset: CoNLL-2003
- Model: Gemini (Pro)