# Lab 3

## ğŸ“ Cáº¥u trÃºc dá»± Ã¡n

```

â”œâ”€â”€ data/                           # Dá»¯ liá»‡u training
â”‚   â”œâ”€â”€ c4-train.00000-of-01024-30K.json.gz
â”‚   â””â”€â”€ en_ewt-ud-train.txt
â”œâ”€â”€ src/                            # Source code chÃ­nh
â”‚   â”œâ”€â”€ preprocessing/
â”‚   â”‚   â””â”€â”€ tokenizer.py           # Tokenizer class
â”‚   â””â”€â”€ representations/
â”‚       â””â”€â”€ word_embedder.py       # WordEmbedder vá»›i pre-trained models
â”œâ”€â”€ test/                           # Scripts demo, test, trá»±c quan hÃ³a
â”‚   â”œâ”€â”€ lab4_test.py               # Test pre-trained embeddings
â”‚   â”œâ”€â”€ lab4_embedding_training_demo.py  # Train vá»›i Gensim
â”‚   â””â”€â”€ lab4_spark_word2vec_demo.py      # Train vá»›i PySpark
â”œâ”€â”€ results/                        # Models Ä‘Ã£ train
â”‚   â”œâ”€â”€ word2vec_ewt.model         # Gensim Word2Vec model
â”‚   â””â”€â”€ spark_word2vec_model/      # Model Spark Word2Vec
â”œâ”€â”€ scripts/                        # Setup scripts (bat, sh, ps1)
â”‚   â”œâ”€â”€ setup_glove.*              # Script táº£i/cÃ i GloVe tÃ¹y mÃ´i trÆ°á»ng
â”‚   â””â”€â”€ cleanup_glove.*            # Script dá»n dáº¹p GloVe tÃ¹y mÃ´i trÆ°á»ng
â”œâ”€â”€ requirements.txt                # Dependencies
â””â”€â”€ README.md                       # TÃ i liá»‡u dá»± Ã¡n
```

## ğŸ”§ Chi tiáº¿t cÃ¡c bÆ°á»›c thá»±c hiá»‡n

### BÆ°á»›c 1: Chuáº©n bá»‹ dá»¯ liá»‡u vÃ  mÃ´i trÆ°á»ng

1. **CÃ i Ä‘áº·t dependencies:**

   ```bash
   pip install -r requirements.txt
   ```

   - CÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t: gensim, pyspark, scikit-learn, matplotlib

2. **Setup GloVe pre-trained model:**

   ```bash
   # Linux/Mac
   bash scripts/setup_glove.sh

   # Windows
   scripts/setup_glove.bat
   ```

### BÆ°á»›c 2: Preprocessing dá»¯ liá»‡u

1. **Tokenization:**

   - File: `src/preprocessing/tokenizer.py`
   - Chá»©c nÄƒng: TÃ¡ch tá»«, loáº¡i bá» stopwords, chuáº©n hÃ³a text

2. **Load Word Embeddings:**

   - File: `src/representations/word_embedder.py`
   - Chá»©c nÄƒng: NhÃºng tá»« sang vector

### BÆ°á»›c 3: Training Word2Vec Models

#### 3.1 Training vá»›i Gensim

**Script:** `test/lab4_embedding_training_demo.py`

**CÃ¡c tham sá»‘ quan trá»ng:**

- `vector_size=100`: KÃ­ch thÆ°á»›c vector embedding
- `window=5`: KÃ­ch thÆ°á»›c context window
- `min_count=1`: Táº§n suáº¥t tá»‘i thiá»ƒu cá»§a tá»«
- `workers=4`: Sá»‘ thread xá»­ lÃ½ song song
- `sg=0`: CBOW (0) hoáº·c Skip-gram (1)

**Quy trÃ¬nh:**

1. Load vÃ  preprocess dá»¯ liá»‡u
2. Khá»Ÿi táº¡o vÃ  train model Word2Vec
3. LÆ°u model vÃ o `results/word2vec_ewt.model`
4. Test similarity giá»¯a cÃ¡c tá»«

#### 3.2 Training vá»›i PySpark

**Script:** `test/lab4_spark_word2vec_demo.py`

**Quy trÃ¬nh:**

1. Khá»Ÿi táº¡o SparkSession
2. Táº¡o DataFrame tá»« dá»¯ liá»‡u Ä‘Ã£ tokenize
3. Sá»­ dá»¥ng `Word2Vec` cá»§a Spark MLlib
4. Training vÃ  lÆ°u model vÃ o `results/spark_word2vec_model/`
5. ÄÃ¡nh giÃ¡ performance

### BÆ°á»›c 4: Sá»­ dá»¥ng Pre-trained Embeddings

**Script:** `test/lab4_test.py`

**Chá»©c nÄƒng:**

1. **Load GloVe vectors:**

   ```python
   from src.representations.word_embedder import WordEmbedder

   embedder = WordEmbedder()
   embedder.load_glove('path/to/glove-wiki-gigaword-50')
   ```

2. **TÃ­nh similarity:**

   ```python
   sim = embedder.similarity('king', 'queen')
   print(f"Similarity: {sim:.4f}")
   ```

3. **TÃ¬m tá»« tÆ°Æ¡ng tá»±:**
   ```python
   similar = embedder.find_similar_words('king', top_k=5)
   ```

### BÆ°á»›c 5: Trá»±c quan hÃ³a Embeddings

**Techniques sá»­ dá»¥ng:**

1. **PCA Dimensionality Reduction:**

   - Giáº£m tá»« 100D xuá»‘ng 2D Ä‘á»ƒ visualization
   - Giá»¯ láº¡i thÃ´ng tin quan trá»ng nháº¥t

2. **Scatter Plot:**

   - Hiá»ƒn thá»‹ cÃ¡c tá»« trong khÃ´ng gian 2D
   - Quan sÃ¡t clustering vÃ  relationships

**LÆ°u Ã½ Spark trÃªn Windows:**

- Cáº§n cÃ i winutils.exe vÃ  thiáº¿t láº­p HADOOP_HOME
- Náº¿u gáº·p lá»—i native Hadoop, xem: https://github.com/steveloughran/winutils

## ğŸ¯ ÄÃ¡nh giÃ¡ vÃ  So sÃ¡nh Models

### 1. Metrics Ä‘Ã¡nh giÃ¡

- **Cosine Similarity:** Äo Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng giá»¯a cÃ¡c word vectors
- **Analogical Reasoning:** Test kháº£ nÄƒng "king - man + woman = queen"
- **Word Similarity Tasks:** Benchmark trÃªn cÃ¡c dataset chuáº©n

### 2. So sÃ¡nh cÃ¡c approaches

| Method                | Pros                         | Cons                                  | Performance  | Use Case              |
| --------------------- | ---------------------------- | ------------------------------------- | ------------ | --------------------- |
| **GloVe Pre-trained** | Cháº¥t lÆ°á»£ng cao, ready-to-use | KÃ­ch thÆ°á»›c lá»›n, fixed vocabulary      | **Tá»‘t nháº¥t** | Production, research  |
| **Spark Word2Vec**    | Scalable, distributed        | Setup phá»©c táº¡p, overhead              | Trung bÃ¬nh   | Big data scenarios    |
| **Gensim Word2Vec**   | Flexible, customizable       | Cáº§n dá»¯ liá»‡u train, thá»i gian training | **KÃ©m nháº¥t** | Domain-specific tasks |

**LÃ½ do hiá»‡u suáº¥t:**

- **GloVe Pre-trained:** ÄÆ°á»£c train trÃªn corpus khá»•ng lá»“ (6B tokens), cÃ³ cháº¥t lÆ°á»£ng semantic representation tá»‘t nháº¥t
- **Gensim Word2Vec:** Train trÃªn dataset nhá» (en_ewt-ud-train.txt), vocabulary háº¡n cháº¿, cháº¥t lÆ°á»£ng phá»¥ thuá»™c vÃ o dá»¯ liá»‡u training

## ğŸ“ CÃ¡ch cháº¡y tá»«ng script

### 1. Test Pre-trained Embeddings

```bash
python test/lab4_test.py
```

**Má»¥c Ä‘Ã­ch:** Test GloVe embeddings, tÃ­nh similarity scores

### 2. Training Demo vá»›i Gensim

```bash
python test/lab4_embedding_training_demo.py
```

**Má»¥c Ä‘Ã­ch:** Train Word2Vec model tá»« scratch, save model

### 3. Training vá»›i Spark

```bash
python test/lab4_spark_word2vec_demo.py
```

**Má»¥c Ä‘Ã­ch:** Distributed training vá»›i Spark MLlib
